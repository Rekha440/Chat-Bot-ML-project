{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual Conversation Application"
      ],
      "metadata": {
        "id": "lQjGBowTn7K2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Made by:\n",
        "\n",
        "Rakhi Kumari\n",
        "\n",
        "M210692CA\n",
        "\n",
        "Machine Learning Assignment"
      ],
      "metadata": {
        "id": "wvXC128NxXTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a contextual conversation bot. This means that the model will be effective within a given context, for example, a ticket booking helper that interacts naturally with a customer and directs them to the appropriate links.\n",
        "\n",
        "The dataset has a number of different `tags` which each correspond to a class of `patterns` and `responses`. When the model detects that an input is provided that resembles the `patterns` within a `tag`, it provides a randomized response from within the `responses` of the same `tag`."
      ],
      "metadata": {
        "id": "a1GYgbjCnXCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program Structure\n"
      ],
      "metadata": {
        "id": "AxDQVvROKJDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset\n",
        "\n",
        "The dataset here is the `intents.json` file. It contains input patterns and corresponding output patterns, grouped under different tags. For example:\n",
        "```\n",
        "{\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\n",
        "        \"Hi\", \"Hey\", \"How are you\",\n",
        "        \"Is anyone there?\", \"Hello\", \"Good day\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Hey :-)\", \"Hello, thanks for visiting\",\n",
        "        \"Hi there, what can I do for you?\", \"Hi there, how can I help?\"\n",
        "      ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "yrBXA81vYiXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Sentence Transformation\n",
        "\n",
        "Each sentence goes through a series of transformations, which is shown in the following example:\n",
        "\n",
        "- Original Sentence: `Hello, thanks for visiting!`\n",
        "\n",
        "- **Tokenization** is the step where all separate words and punctuations in the sentence get separated into a list of words, punctuations. The previous sentence after tokenization becomes `['Hello', ',', 'thanks', 'for', 'visiting', '!']`\n",
        "\n",
        "- **Stemming** is a process where the suffixes are removed from words, and the words are transformed to their root form. For example, `playing`, `played`, `player`, `plays` all get converted to `play`. The previous list after converting to lowercase and stemming becomes `['hello', ',', 'thank', 'for', 'visit', '!']`\n",
        "\n",
        "- Punctuations are also removed `['hello', 'thank', 'for', 'visit']`\n",
        "\n",
        "- Finally, the list of words is converted into a **bag of words**. This is a sort of one-hot encoding based on *all* the words in the dataset. This conversion is explained in more detail further down. This sentence converted to bag of words would be `[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0]`"
      ],
      "metadata": {
        "id": "iQ5arIpPYkES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of Words Algorithm\n",
        "\n",
        "Consider the dictionary of words for the whole project:\n",
        "\n",
        "`all_words = ['hi', 'hello', 'welcome', 'for', 'good', 'i', 'you', 'bye', 'thank', 'cool', 'visit']`\n",
        "\n",
        "`sentence = ['hello', 'thank', 'for', 'visit']`\n",
        "\n",
        "The previous stemmed sentence converted to bag of words will be:\n",
        "\n",
        "`[0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1]`\n",
        "\n",
        "The words present in the sentence are marked as `1`, all other words are marked as `0`."
      ],
      "metadata": {
        "id": "Es3y-DU6gHq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create Training Data\n",
        "\n",
        "We use the dataset, and transform the dataset as mentioned above to create our training data.\n",
        "\n",
        "- **Tokenize** each sentence in the dataset, **Stem** each word and add all such words to the `all_words` list. The list should be in alphabetical order. `[\"'s\", 'a', 'accept', 'anyon', 'are', 'bye',..., 'which', 'with', 'you']`\n",
        "\n",
        "- For each input `patterns` in the dataset, **tokenize** and store them along with their corresponding `tags`. Each pattern converted to bag of words becomes the `x_train`. The corresponding index for each `tag` becomes the `y_train`.\n",
        "\n",
        "  ```\n",
        "x_train:\n",
        "[[0. 0. 0. ... 0. 0. 0.]\n",
        " [1. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 1.]\n",
        " ...\n",
        " [0. 1. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 1.]]\n",
        "  ```\n",
        "  ```\n",
        "y_train:\n",
        "[3 3 3 3 3 3 2 2 2 6 6 6 6 4 4 4 5 5 5 5 0 0 0 1 1 1 1 1 1 1]\n",
        "\n",
        "  ```\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "G28OGfsNhV0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Create Model with Training Data\n",
        "\n",
        "Here we use a Feed-Forward Neural Network, which is a type of Artificial Neural Network (ANN). For the algorithm, we use the `pytorch` library.\n",
        "\n",
        "The Neural Network here uses an input layer, two hidden layers and an output layer. The input layer takes any sentence as an input, and the output layer returns the `tag` which has the highest probability of being a match with the input. It actually returns the probability of the input belonging to each `tag`.\n",
        "\n",
        "For example, an input of `Hello there` would return the tag `greeting`.\n",
        "\n",
        "After training the model, we store the trained model into a file `data.pth`, which is a `Pytorch` trained model. We use this `data.pth` to generate responses."
      ],
      "metadata": {
        "id": "JMYphsHRnN-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Using the Model\n",
        "\n",
        "To use the model we use the following steps:\n",
        "- Get an input sentence from the user, and convert it into bag of words.\n",
        "- Feed the transformed sentence into the model, which returns a `tag`.\n",
        "- Find the corresponding `response` to the `tag` from the dataset, and randomly select and return a `response`."
      ],
      "metadata": {
        "id": "dzZt8jLPz-4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Qvg4OSKiKczf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\n",
        "        \"Hi\",\n",
        "        \"Hey\",\n",
        "        \"How are you\",\n",
        "        \"Is anyone there?\",\n",
        "        \"Hello\",\n",
        "        \"Good day\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Hey :-)\",\n",
        "        \"Hello, thanks for visiting\",\n",
        "        \"Hi there, what can I do for you?\",\n",
        "        \"Hi there, how can I help?\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"goodbye\",\n",
        "      \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
        "      \"responses\": [\n",
        "        \"See you later, thanks for visiting\",\n",
        "        \"Have a nice day\",\n",
        "        \"Bye! Come back again soon.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"thanks\",\n",
        "      \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\", \"Thank's a lot!\"],\n",
        "      \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"items\",\n",
        "      \"patterns\": [\n",
        "        \"Which items do you have?\",\n",
        "        \"What kinds of items are there?\",\n",
        "        \"What do you sell?\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"We sell coffee and tea\",\n",
        "        \"We have coffee and tea\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"payments\",\n",
        "      \"patterns\": [\n",
        "        \"Do you take credit cards?\",\n",
        "        \"Do you accept Mastercard?\",\n",
        "        \"Can I pay with Paypal?\",\n",
        "        \"Are you cash only?\",\n",
        "        \"How much is it?\",\n",
        "        \"What payment methods do you have?\",\n",
        "        \"How do I pay?\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"We accept VISA, Mastercard and Paypal\",\n",
        "        \"We accept most major credit cards, and Paypal\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"delivery\",\n",
        "      \"patterns\": [\n",
        "        \"How long does delivery take?\",\n",
        "        \"How long does shipping take?\",\n",
        "        \"When do I get my delivery?\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Delivery takes 2-4 days\",\n",
        "        \"Shipping takes 2-4 days\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"funny\",\n",
        "      \"patterns\": [\n",
        "        \"Tell me a joke!\",\n",
        "        \"Tell me something funny!\",\n",
        "        \"Do you know a joke?\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"I once beat a human at chess. But I was no match for him at kickboxing.\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "NAqBOY7MKkX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files\n",
        "\n",
        "`nltk_util.py`"
      ],
      "metadata": {
        "id": "jK2uCVwvKp1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return np.array(nltk.word_tokenize(sentence))\n",
        "\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "    for i in range(tokenized_sentence.shape[0]):\n",
        "        tokenized_sentence[i] = stem(tokenized_sentence[i])\n",
        "\n",
        "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
        "    \n",
        "    for i in range(len(all_words)):\n",
        "        if all_words[i] in tokenized_sentence:\n",
        "            bag[i] = 1.0\n",
        "\n",
        "    return bag"
      ],
      "metadata": {
        "id": "dkw5MY9GyvhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.py`"
      ],
      "metadata": {
        "id": "_SME19970zeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Layer 1. Input into First hidden layer\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Layer 2. Output from first hidden layer into second hidden layer.\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Layer 3. Output from second hidden layer to output layer\n",
        "        out = self.l3(out)\n",
        "        # Outputs are further converted to cross-entropy loss in optimization loop\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "sHcBCfGR0zFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train.py`"
      ],
      "metadata": {
        "id": "xRMW-6wOzA7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk_util import tokenize, stem, bag_of_words\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from model import NeuralNet\n",
        "\n",
        "\n",
        "with open('intents.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "\n",
        "# Step 1: Create List of all Words\n",
        "# Also stores each sentence in patterns along with corresponding tag in 'xy'\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "# List of punctuations to be ignored\n",
        "ignore_words = ['?', '!', ',', '.', ';']\n",
        "\n",
        "\n",
        "# Removes punctuations from all_words\n",
        "temp = []\n",
        "for word in all_words:\n",
        "    if word not in ignore_words:\n",
        "        temp.append(stem(word))\n",
        "all_words = np.array(temp)    \n",
        "\n",
        "# Sorts tags and all_words in ascending order\n",
        "all_words = sorted(np.unique(all_words))\n",
        "tags = sorted(np.unique(tags))\n",
        "\n",
        "\n",
        "# Create training data\n",
        "x_train = []    # Stores the sentence vector (1-hot encoded, sort of)\n",
        "y_train = []    # Stores the index of the corresponding tag.\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    x_train.append(bag)\n",
        "\n",
        "    label = tags.index(tag)     # Tags is a list. This line finds the index of `tag` in that list.\n",
        "    y_train.append(label)       \n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size = 8\n",
        "hidden_size = 8     # Number of nodes in hidden layer\n",
        "input_size = len(x_train[0])    # Each input is an encoded sentence.\n",
        "output_size = len(tags)\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000   # Maximum number of iterations for optimization\n",
        "\n",
        "\n",
        "# This class is used to transform the training data into data that can be used as Pytorch NN input\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.n_samples = len(x_train)\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "\n",
        "    # Allows us to access a dataset with an index\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "    def __str__(self):\n",
        "        string = f'{self.x_data}\\n{self.y_data}'\n",
        "        return string\n",
        "\n",
        "\n",
        "# Takes training data and transforms it into NN input\n",
        "dataset = ChatDataset(x_train, y_train)\n",
        "train_loader = DataLoader(dataset = dataset, batch_size = batch_size, \n",
        "                          shuffle = True, num_workers = 0)\n",
        "\n",
        "# Checks if GPU available, else uses CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()   # Loss Function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)    # Optimization Function\n",
        "\n",
        "\n",
        "# Optimization Loop\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and Optimizer Step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'epoch = {epoch + 1}/{num_epochs}, loss = {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Here we will store the trained model into a file,\n",
        "# so that we won't have to train the model everytime we use the bot.\n",
        "# Data that will be stored in this dictionary format\n",
        "data = {\n",
        "    'model_state': model.state_dict(),\n",
        "    'input_size': input_size,\n",
        "    'output_size': output_size,\n",
        "    'hidden_size': hidden_size,\n",
        "    'all_words': all_words,\n",
        "    'tags': tags\n",
        "}\n",
        "\n",
        "FILE = 'data.pth'\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'Training complete. File saved to {FILE}')"
      ],
      "metadata": {
        "id": "6oEDTaw7y_6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`chat.py`"
      ],
      "metadata": {
        "id": "qOvrqw-q2QdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from model import NeuralNet\n",
        "from nltk_util import bag_of_words, tokenize\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Import dataset\n",
        "# The dataset is used here to match output tag with corresponding responses.\n",
        "with open('intents.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "\n",
        "# Import pre trained model\n",
        "FILE = 'data.pth'\n",
        "data = torch.load(FILE)\n",
        "\n",
        "# Retrieve trained data from 'data.pth' file\n",
        "input_size = data['input_size']\n",
        "hidden_size = data['hidden_size']\n",
        "output_size = data['output_size']\n",
        "all_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data['model_state']\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "bot_name = 'Bot'\n",
        "print(\"Let's chat! Type 'quit' to Quit\")\n",
        "while True:\n",
        "    sentence = input('You: ')\n",
        "    if sentence == 'quit':\n",
        "        break\n",
        "\n",
        "    sentence = tokenize(sentence)\n",
        "    x = bag_of_words(sentence, all_words)\n",
        "    x = x.reshape(1, x.shape[0])\n",
        "    x = torch.from_numpy(x)\n",
        "\n",
        "    output = model(x)\n",
        "    _, predicted = torch.max(output, dim = 1)\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    # Cross entropy loss is converted to probability using softmax function\n",
        "    probs = torch.softmax(output, dim = 1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent['tag']:\n",
        "                response_choice = random.choice(intent['responses'])\n",
        "                print(f'{bot_name}: {response_choice}')\n",
        "    else:\n",
        "        print(f'{bot_name}: I do not understand...')"
      ],
      "metadata": {
        "id": "o0Mj_4Q22JHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}